{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "text_complex = \"In information theory, the Rényi entropy is a quantity that generalizes various notions of entropy, including Hartley entropy, Shannon entropy, collision entropy, and min-entropy. The Rényi entropy is named after Alfréd Rényi, who looked for the most general way to quantify information while preserving additivity for independent events. In the context of fractal dimension estimation, the Rényi entropy forms the basis of the concept of generalized dimensions. As α approaches zero, the Rényi entropy increasingly weighs all events with nonzero probability more equally, regardless of their probabilities. In the limit for α → 0, the Rényi entropy is just the logarithm of the size of the support of X. The limit for α → 1 is the Shannon entropy. As α approaches infinity, the Rényi entropy is increasingly determined by the events of highest probability.\"\n",
    "text_simple = \"The Rényi entropy is a type of entropy that measures information. It is named after Alfréd Rényi, who wanted to find a way to measure information that was general and worked for different types of events. The Rényi entropy is used in fractal dimension estimation. As the value of alpha changes, the Rényi entropy values different events differently. When alpha is very small, the Rényi entropy values all events equally. When alpha is very large, the Rényi entropy values the events with the highest probability the most. When alpha is 1, the Rényi entropy is the same as the Shannon entropy.\"\n",
    "\n",
    "tok_complex = tokenizer.encode(text_complex, return_tensors='tf')\n",
    "tok_simple = tokenizer.encode(text_simple, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking')\n",
    "\n",
    "text_complex = \"In information theory, the Rényi entropy is a quantity that generalizes various notions of entropy, including Hartley entropy, Shannon entropy, collision entropy, and min-entropy. The Rényi entropy is named after Alfréd Rényi, who looked for the most general way to quantify information while preserving additivity for independent events. In the context of fractal dimension estimation, the Rényi entropy forms the basis of the concept of generalized dimensions. As α approaches zero, the Rényi entropy increasingly weighs all events with nonzero probability more equally, regardless of their probabilities. In the limit for α → 0, the Rényi entropy is just the logarithm of the size of the support of X. The limit for α → 1 is the Shannon entropy. As α approaches infinity, the Rényi entropy is increasingly determined by the events of highest probability.\"\n",
    "text_simple = \"The Rényi entropy is a type of entropy that measures information. It is named after Alfréd Rényi, who wanted to find a way to measure information that was general and worked for different types of events. The Rényi entropy is used in fractal dimension estimation. As the value of alpha changes, the Rényi entropy values different events differently. When alpha is very small, the Rényi entropy values all events equally. When alpha is very large, the Rényi entropy values the events with the highest probability the most. When alpha is 1, the Rényi entropy is the same as the Shannon entropy.\"\n",
    "\n",
    "# out = unmasker(\"Hello I'm a [MASK] model.\")\n",
    "# print(out)\n",
    "\n",
    "def compute_complexity_single(text, word):\n",
    "    out = unmasker(text)\n",
    "    out_matching = [x for x in out if x[\"token_str\"] == word]\n",
    "    prob_top = out[0][\"score\"]\n",
    "    return prob_top, len(out_matching)\n",
    "\n",
    "def compute_complexity(text):\n",
    "    text = text.split(\" \")\n",
    "    prob_top_all = []\n",
    "    matched_all = []\n",
    "    for i in tqdm.tqdm(range(len(text))):\n",
    "        text_new = \" \".join(text[:i] + [\"[MASK]\"] + text[i+1:])\n",
    "        prob_top, matched = compute_complexity_single(text_new, text[i])\n",
    "        prob_top_all.append(prob_top)\n",
    "        matched_all.append(matched)\n",
    "    print(f\"Prob: {np.average(prob_top_all):.2f}\")\n",
    "    print(f\"Matched: {np.average(matched):.5%}\")\n",
    "\n",
    "compute_complexity(text_complex)\n",
    "compute_complexity(text_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "\n",
    "\n",
    "# Set up input text\n",
    "input_text = \"Your existing text here.\"\n",
    "\n",
    "# Encode input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output   \n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=1,\n",
    "    # max_length=len(input_ids[0]),\n",
    "    # do_sample=False,\n",
    "    # num_beams=1,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "print(output[\"scores\"])\n",
    "\n",
    "# Get token scores\n",
    "# token_scores = output[0][0].tolist()\n",
    "# decoded_tokens = tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complexity_2(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1,\n",
    "        # max_length=len(input_ids[0]),\n",
    "        # do_sample=False,\n",
    "        # num_beams=1,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    print(output['scores'][0][0])\n",
    "    print(f\"Prob: {np.average(output['scores'][0][0]):.2f}\")\n",
    "\n",
    "compute_complexity_2(text_complex)\n",
    "compute_complexity_2(text_simple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
